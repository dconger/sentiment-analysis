{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "### Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Something about how Koresh had threatened to cause local \\nproblems with all these wepaons he had and was alleged to\\nhave.  \\n\\nSomeone else will post more details soon, I'm sure.\\n\\nOther News:\\nSniper injures 9 outside MCA buildling in L.A.  Man arrested--suspect\\nwas disgruntled employee of Universal Studios, which\\nis a division of M.C.A.\\n\\n\\nQUESTION:\\nWhat will Californians do with all those guns after the Reginald\\ndenny trial?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=2,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "text_samples = dataset.data[:1000]\n",
    "text = text_samples[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "stems = [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "went\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "print lemmatizer.lemmatize('go', pos='v') \n",
    "print lemmatizer.lemmatize('went') \n",
    "print lemmatizer.lemmatize('went', pos='v')\n",
    "\n",
    "# The lemmatize() function takes a second optional argument, which is the POS tag. Without the POS tag information, the lemmatizer is likely to fail, as follows:\n",
    "\n",
    "# lemmatizer.lemmatize('am') \n",
    "# lemmatizer.lemmatize('am', pos='v') \n",
    "# lemmatizer.lemmatize('is') \n",
    "# lemmatizer.lemmatize('is', pos='v') \n",
    "# lemmatizer.lemmatize('are')  \n",
    "# lemmatizer.lemmatize('are', pos='v') \n",
    "\n",
    "# The available POS tags for the WordNet-based lemmatizer are grouped into macro-categories: adjectives (a), nouns (n), verbs (v), and adverbs (r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('short', 'JJ'),\n",
       " (',', ','),\n",
       " ('nice', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('point', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(\"This sentence is short, nice and to the point\") \n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>areas</th>\n",
       "      <th>aren</th>\n",
       "      <th>argic</th>\n",
       "      <th>argument</th>\n",
       "      <th>armenian</th>\n",
       "      <th>armenians</th>\n",
       "      <th>arms</th>\n",
       "      <th>army</th>\n",
       "      <th>article</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   areas  aren  argic  argument  armenian  armenians  arms  army  article  \\\n",
       "0      0     0      0         0         0          0     0     0        0   \n",
       "1      0     0      0         0         0          0     0     0        0   \n",
       "2      0     0      0         0         0          0     0     0        0   \n",
       "3      0     0      0         0         0          0     0     0        0   \n",
       "4      0     0      0         0         0          0     0     0        0   \n",
       "5      0     0      1         0         5          4     0     0        0   \n",
       "6      0     0      0         0         0          0     0     0        0   \n",
       "\n",
       "   articles  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "5         0  \n",
       "6         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "# min_df: Ignore terms that have doc freq lower than the given threshold\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000,\n",
    "                                stop_words='english')\n",
    "tf_matrix = tf_vectorizer.fit_transform(text_samples)\n",
    "tf_vocab = tf_vectorizer.get_feature_names()\n",
    "\n",
    "i = 130\n",
    "j = 10\n",
    "tf_sample_words = tf_vectorizer.get_feature_names()[i:i+10]\n",
    "tf_sample_words\n",
    "pandas.DataFrame(tf_matrix[j:j+7,i:i+10].todense(), columns=tf_sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x108 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 25613 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000,\n",
    "                                 min_df=0.1)\n",
    "                                 #use_idf=True, tokenizer=nltk.word_tokenize, ngram_range=(1,3)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_samples) #fit the vectorizer to synopses\n",
    "tfidf_vocab = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# The type of input that Word2Vec is looking for.. \n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_samples]\n",
    "\n",
    "model = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:11, index:1408, sample_int:4294967296)\n",
      "Vocab(count:6, index:2670, sample_int:4294967296)\n",
      "Vocab(count:5, index:3018, sample_int:4294967296)\n",
      "Vocab(count:9, index:1696, sample_int:4294967296)\n",
      "[(u'engine', 0.9990893006324768), (u'explained', 0.9990804195404053), (u'property', 0.9990782141685486), (u'swap', 0.9990752935409546)]\n",
      "0.997880684459\n",
      "0.998612564475\n",
      "koresh\n"
     ]
    }
   ],
   "source": [
    "# Some useful attributes\n",
    "print model.vocab[u'koresh']\n",
    "print model.vocab[u'cult']\n",
    "print model.vocab[u'waco']\n",
    "print model.vocab[u'motherboard']\n",
    "print model.most_similar('usenet' ,topn=4)\n",
    "print model.similarity('woman','man')\n",
    "print model.n_similarity(['woman', 'girl'], ['man', 'boy'])\n",
    "print model.doesnt_match(\"waco koresh cult motherboard\".split()) # this doesn't work very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Dimensionality Reduction\n",
    "## LDA: Latent Drichlet Analysis\n",
    "\n",
    "\n",
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x12404ac90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "def lda_model(docs):\n",
    "    # Build LDA model, setting the number of topics to extract\n",
    "    return LdaModel(docs, num_topics=20)\n",
    "\n",
    "def lda_vector(lda_model, doc):\n",
    "    # Generate features for a new document\n",
    "    return lda_model[doc]\n",
    "\n",
    "from gensim.utils import mock_data\n",
    "gensim_corpus = mock_data()\n",
    "lda = lda_model(gensim_corpus)\n",
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def latent_semantic_analysis(docs):\n",
    "    tfidf = TfidfVectorizer() # Using default parameters\n",
    "    tfidf.fit(docs) # Creating dictionary\n",
    "    vecs = tfidf.transform(docs) # Using dictionary to vectorize documents\n",
    "    svd = TruncatedSVD(n_components=100) # Generating 100 top components\n",
    "    svd.fit(vecs) # Creating SVD matrices\n",
    "    return svd.transform(vecs) # Finally use LSA to vectorize documents\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "latent_semantic_analysis(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "X = pca.fit_transform(tfidf_matrix.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "X = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
